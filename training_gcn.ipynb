{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "840f1a55-6e23-4e19-b78e-732c782ef318",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import re\n",
    "from argparse import ArgumentParser\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import pytorch_lightning as pl\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torchmetrics.functional as metrics\n",
    "\n",
    "from dgl.nn import GraphConv\n",
    "import dgllife as life\n",
    "from dgllife.utils import ConcatFeaturizer\n",
    "import datamol as dm\n",
    "from multiprocessing import Manager\n",
    "from rdkit.rdBase import BlockLogs\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdchem\n",
    "import subprocess\n",
    "\n",
    "import wandb\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "# Import GPU-related things\n",
    "if torch.cuda.is_available():\n",
    "    # import cupy as np\n",
    "    # import cudf as pd\n",
    "\n",
    "    # Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "    torch.backends.cudnn.determinstic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    DEVICE = torch.device(\"cuda:0\")\n",
    "# else:\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = os.environ.get(\"PATH_DATASETS\", \"data/\")\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"saved_models/\")\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "print('CUDA:', torch.cuda.is_available())\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "temp_path = './temp'\n",
    "data_path = './data'\n",
    "\n",
    "if not os.path.exists(temp_path):\n",
    "    os.mkdir(temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a245efbf-93d3-4502-9142-8ce1c2d9b1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total xyz filepath #  133885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./data/dsgdb9nsd_000001.xyz'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xyz_filepath_list = list(glob(f'{data_path}/*.xyz'))\n",
    "\n",
    "xyz_filepath_list.sort()\n",
    "print('total xyz filepath # ', len(xyz_filepath_list))\n",
    "xyz_filepath_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89c31f7a-106a-4ba0-ac38-1052c47a61cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_ipython():\n",
    "    try:\n",
    "        return __IPYTHON__\n",
    "    except NameError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e6174fc-100e-4977-87a5-58176a50bc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40b706e6-ca46-4243-b035-d111d510ebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaff2_types = df['type'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad3d3234-aaf1-4761-8dba-c2cb4a31c738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c3',\n",
       " 'hc',\n",
       " 'n9',\n",
       " 'hn',\n",
       " 'oh',\n",
       " 'ho',\n",
       " 'n3',\n",
       " 'h3',\n",
       " 'os',\n",
       " 'h2',\n",
       " 'h1',\n",
       " 'n8',\n",
       " 'cx',\n",
       " 'op',\n",
       " 'n7',\n",
       " 'np',\n",
       " 'cy',\n",
       " 'oq',\n",
       " 'nz',\n",
       " 'o',\n",
       " 'hx',\n",
       " 'n6',\n",
       " 'n5',\n",
       " 'f',\n",
       " 'ny',\n",
       " 'nq',\n",
       " 'nx',\n",
       " 'nk',\n",
       " 'nl',\n",
       " 'nh',\n",
       " 'n2',\n",
       " 'c2',\n",
       " 'ha',\n",
       " 'nu',\n",
       " 'h4',\n",
       " 'cu',\n",
       " 'cv',\n",
       " 'n4']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaff2_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d970c3-5e02-4102-b192-c616c0526e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0d10b5-b086-457d-9cdd-f2eae6cf83ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subfeatures(num_features, divider):\n",
    "    return int(np.floor(num_features / divider))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2411315f-2495-4972-a0cd-31b9f8da6a5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FFNetwork(pl.LightningModule):\n",
    "    def __init__(self, num_features, num_classes, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.sequential = nn.Sequential(\n",
    "            GraphConv(self.num_features, get_subfeatures(self.num_features, 2)),\n",
    "            nn.ReLU(),\n",
    "            GraphConv(get_subfeatures(self.num_features, 2), self.num_classes)\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._calculate_loss(batch, mode=\"train\")\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"test\")\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.sequential(X)\n",
    "\n",
    "    def _calculate_loss(self, batch, mode=\"train\"):\n",
    "        X, y = batch\n",
    "\n",
    "        preds = self.forward(X)\n",
    "        loss = F.cross_entropy(preds, y)\n",
    "\n",
    "        # Logging to WANDB\n",
    "        self.log(f\"{mode}_loss\", loss)\n",
    "        self.log(f'{mode}_acc', metrics.accuracy(preds, y.long(), average='macro', num_classes=self.num_classes), prog_bar=True)\n",
    "        self.log(f'{mode}_f1', metrics.f1_score(preds, y.long(), average='macro', num_classes=self.num_classes), prog_bar=True)\n",
    "        self.log(f'{mode}_prc', metrics.precision(preds, y.long(), average='macro', num_classes=self.num_classes), prog_bar=False)\n",
    "        self.log(f'{mode}_rcl', metrics.recall(preds, y.long(), average='macro', num_classes=self.num_classes), prog_bar=False)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=0.001,\n",
    "            weight_decay=0.03\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return data.DataLoader(\n",
    "            CustomDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float()),\n",
    "            batch_size=64,\n",
    "            shuffle=True,\n",
    "            num_workers=8\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return data.DataLoader(\n",
    "            CustomDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).float()),\n",
    "            batch_size=1,\n",
    "            num_workers=8\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return data.DataLoader(\n",
    "            CustomDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float()),\n",
    "            batch_size=1,\n",
    "            num_workers=8\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fefb75-5647-420f-86de-9c82171de8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "def main(hparams):\n",
    "    # wandb.finish()\n",
    "    # wandb_logger = WandbLogger(project=\"bachelor\")\n",
    "\n",
    "    neptune_logger = NeptuneLogger(\n",
    "        project=\"caigh/bachelor\",\n",
    "        api_key=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI4NjQyYTAwMy1jMmJkLTRjMzctYWQ0Zi03Y2FmYjQ1YmQ2Y2MifQ==\",\n",
    "        log_model_checkpoints=True,\n",
    "    )\n",
    "\n",
    "    neptune_logger.finalize('success')\n",
    "\n",
    "    print('Loading data..')\n",
    "    print(f'X shape: {X.shape[1]}')\n",
    "    print(f'y shape: {y.shape[1]}')\n",
    "\n",
    "    model = FFNetwork(\n",
    "        num_features=X.shape[1],\n",
    "        num_classes=y.shape[1],\n",
    "        dropout=0.5\n",
    "    )\n",
    "\n",
    "    # train the model\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator='gpu',\n",
    "        strategy='dp',\n",
    "        max_epochs=10,\n",
    "        min_epochs=1,\n",
    "        # overfit_batches=1,\n",
    "        logger=neptune_logger,\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor=\"train_loss\", mode=\"min\"),\n",
    "            ModelCheckpoint(dirpath='./checkpoints', filename='{epoch}-{val_loss:.2f}-{val_f1:.2f}', monitor='val_f1', save_last=True),\n",
    "        ],\n",
    "     )\n",
    "\n",
    "    trainer.fit(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "165cf464-e658-4d44-b207-7b5bc7a2526c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcaigh\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20220919_233117-2rm594y9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/caigh/bachelor/runs/2rm594y9\" target=\"_blank\">classification_14</a></strong> to <a href=\"https://wandb.ai/caigh/bachelor\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'checkpoint_callback'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m     main(hyperparams)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [6], line 40\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(hparams)\u001b[0m\n\u001b[1;32m     25\u001b[0m experiment_config \u001b[38;5;241m=\u001b[39m ExperimentConfig(\n\u001b[1;32m     26\u001b[0m     project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbachelor\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     27\u001b[0m     exp_watch\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgradients\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     28\u001b[0m     log_target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwandb\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     29\u001b[0m     log_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m tabular_model \u001b[38;5;241m=\u001b[39m TabularModel(\n\u001b[1;32m     33\u001b[0m     data_config\u001b[38;5;241m=\u001b[39mdata_config,\n\u001b[1;32m     34\u001b[0m     model_config\u001b[38;5;241m=\u001b[39mmodel_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     experiment_config\u001b[38;5;241m=\u001b[39mexperiment_config\n\u001b[1;32m     38\u001b[0m )\n\u001b[0;32m---> 40\u001b[0m \u001b[43mtabular_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m result \u001b[38;5;241m=\u001b[39m tabular_model\u001b[38;5;241m.\u001b[39mevaluate(test)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[0;32m~/miniconda3/envs/mm/lib/python3.9/site-packages/pytorch_tabular/tabular_model.py:431\u001b[0m, in \u001b[0;36mTabularModel.fit\u001b[0;34m(self, train, validation, test, loss, metrics, optimizer, optimizer_params, train_sampler, target_transform, max_epochs, min_epochs, reset, seed)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;124;03m\"\"\"The fit method which takes in the data and triggers the training\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03m    seed: (int): If you have to override the default seed set as part of of ModelConfig\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    430\u001b[0m seed_everything(seed \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m--> 431\u001b[0m train_loader, val_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pre_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mauto_lr_find \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mfast_dev_run):\n",
      "File \u001b[0;32m~/miniconda3/envs/mm/lib/python3.9/site-packages/pytorch_tabular/tabular_model.py:376\u001b[0m, in \u001b[0;36mTabularModel._pre_fit\u001b[0;34m(self, train, validation, test, loss, metrics, optimizer, optimizer_params, train_sampler, target_transform, max_epochs, min_epochs, reset)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mwatch(\n\u001b[1;32m    373\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mexp_watch, log_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mexp_log_freq\n\u001b[1;32m    374\u001b[0m     )\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_callbacks()\n\u001b[0;32m--> 376\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_loader, val_loader\n",
      "File \u001b[0;32m~/miniconda3/envs/mm/lib/python3.9/site-packages/pytorch_tabular/tabular_model.py:310\u001b[0m, in \u001b[0;36mTabularModel._prepare_trainer\u001b[0;34m(self, max_epochs, min_epochs)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# For some weird reason, checkpoint_callback is not appearing in the Trainer vars\u001b[39;00m\n\u001b[1;32m    309\u001b[0m trainer_args_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint_callback\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mcheckpoint_callback\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrainer_args_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mm/lib/python3.9/site-packages/pytorch_lightning/utilities/argparse.py:345\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mlist\u001b[39m(env_variables\u001b[38;5;241m.\u001b[39mitems()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mitems()))\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# all args were already moved to kwargs\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'checkpoint_callback'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if not in_ipython():\n",
    "        root_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "        parser = ArgumentParser(add_help=False)\n",
    "        hyperparams = parser.parse_args()\n",
    "\n",
    "        # TRAIN\n",
    "        main(hyperparams)\n",
    "    else:\n",
    "        main(None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
